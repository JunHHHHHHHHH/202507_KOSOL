# rag_logic.py

import os
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

# --- ìƒìˆ˜ ì •ì˜ ---
# í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ ë””ë ‰í† ë¦¬ì— PDF íŒŒì¼ì´ ìˆë„ë¡ ê²½ë¡œ ì„¤ì •
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
PDF_PATH = os.path.join(BASE_DIR, "ì£¼ê°„ë†ì‚¬ì •ë³´ ì œ28í˜¸.pdf") #PDF_PATH = os.path.join(BASE_DIR, "ì£¼ê°„ë†ì‚¬ì •ë³´ ì œ28í˜¸.pdf")

EMBEDDING_MODEL_NAME = "nomic-embed-text"
LLM_MODEL_NAME = "gemma3"  # llama3ë¡œ ë³€ê²½ ê°€ëŠ¥


# RAG ì²´ì¸ì„ ì´ˆê¸°í™”í•˜ê³  ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
def initialize_rag_chain():
    """
    ë¬¸ì„œ ë¡œë“œ, ë¶„í• , ì„ë² ë”©, ë²¡í„°DB ìƒì„±, RAG ì²´ì¸ ìƒì„±ì„ ìˆ˜í–‰í•˜ê³ 
    ìƒì„±ëœ RAG ì²´ì¸ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    print("--- RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘ ---")
    
    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists(PDF_PATH):
        raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {PDF_PATH}")
    
    print(f"ğŸ“„ PDF íŒŒì¼ ê²½ë¡œ: {PDF_PATH}")
    
    # 1. ë¬¸ì„œ ë¡œë“œ
    loader = PyPDFLoader(PDF_PATH)
    docs = loader.load()
    print("âœ… [1/5] ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    
    # 2. ë¬¸ì„œ ë¶„í• 
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_documents(docs)
    print("âœ… [2/5] ë¬¸ì„œ ë¶„í•  ì™„ë£Œ")
    
    # 3. ì„ë² ë”© ëª¨ë¸ ë° ë²¡í„° DB ì„¤ì •
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)
    print("âœ… [3/5] FAISS ë²¡í„° DB ìƒì„± ì™„ë£Œ")
    
    # 4. ê²€ìƒ‰ê¸° ìƒì„±
    retriever = vectorstore.as_retriever()
    print("âœ… [4/5] ê²€ìƒ‰ê¸° ìƒì„± ì™„ë£Œ")
    
    # 5. í”„ë¡¬í”„íŠ¸ ë° LLM ì„¤ì •, RAG ì²´ì¸ ìƒì„±
    template = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ë¬¸ë§¥(context)ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œë§Œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ì‹¤ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë¬¸ë§¥ì—ì„œ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ "ë¬¸ë§¥ì—ì„œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”. ëª¨ë“  ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜.

CONTEXT: {context}

QUESTION: {question}
"""
    
    prompt = ChatPromptTemplate.from_template(template)
    llm = ChatOllama(model=LLM_MODEL_NAME, temperature=0)
    
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    
    print("âœ… [5/5] RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
    print("--- RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ ---")
    
    return rag_chain

# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_answer(chain, question):
    """
    ì´ˆê¸°í™”ëœ RAG ì²´ì¸ê³¼ ì§ˆë¬¸ì„ ë°›ì•„ ë‹µë³€ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    return chain.invoke(question)
